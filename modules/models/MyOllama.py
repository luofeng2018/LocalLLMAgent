# å¯¼å…¥æ‰€éœ€åº“
import socket

import requests  # ç”¨äºå‘é€HTTPè¯·æ±‚
import json  # ç”¨äºJSONæ•°æ®è§£æå’Œåºåˆ—åŒ–
from typing import List, Tuple, Generator, Dict, Any, Optional  # ç±»å‹æç¤ºæ”¯æŒ


class OllamaClient:
    """Ollama APIå®¢æˆ·ç«¯å°è£…"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯

        Args:
            base_url: APIåŸºç¡€URL (é»˜è®¤ http://localhost:11434)
        """
        self.base_url = base_url  # å­˜å‚¨APIåŸºç¡€URL
        self.models_endpoint = f"{base_url}/api/tags"  # æ¨¡å‹åˆ—è¡¨APIç«¯ç‚¹
        self.chat_endpoint = f"{base_url}/api/chat"  # èŠå¤©APIç«¯ç‚¹

    def get_local_models(self) -> List[str]:
        """è·å–æœ¬åœ°å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            # å‘é€GETè¯·æ±‚è·å–æ¨¡å‹åˆ—è¡¨
            response = requests.get(self.models_endpoint)
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            data = response.json()  # è§£æJSONå“åº”
            # ä»å“åº”ä¸­æå–æ¨¡å‹åç§°åˆ—è¡¨
            return [model['name'] for model in data.get('models', [])]
        except requests.exceptions.RequestException as e:
            # ç½‘ç»œè¯·æ±‚é”™è¯¯å¤„ç†
            print(f"æ¨¡å‹è·å–å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
            return ["llama3", "mistral"]
        except (json.JSONDecodeError, KeyError) as e:
            # JSONè§£ææˆ–é”®å€¼é”™è¯¯å¤„ç†
            print(f"å“åº”è§£æå¤±è´¥: {str(e)}")
            return []

    def generate_response(
            self,
            message: str,  # ç”¨æˆ·å½“å‰è¾“å…¥
            history: List[Tuple[str, str]],  # å¯¹è¯å†å²è®°å½•
            model_name: str,  # ä½¿ç”¨çš„æ¨¡å‹åç§°
            use_stream: bool = True  # æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
    ) -> Generator[str, None, None]:
        """
        ç”Ÿæˆæ¨¡å‹å“åº” (æ”¯æŒæµå¼/éæµå¼)

        Args:
            message: ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            history: å¯¹è¯å†å² [(ç”¨æˆ·æ¶ˆæ¯, AIå›å¤), ...]
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”

        Yields:
            AIå“åº”å†…å®¹ (åˆ†å—æˆ–å®Œæ•´)
        """
        # æ ¼å¼åŒ–å¯¹è¯å†å²ä¸ºAPIæ‰€éœ€çš„æ ¼å¼
        messages = self._format_messages(message, history)

        # æ„é€ APIè¯·æ±‚è´Ÿè½½
        payload = {
            "model": model_name,
            "messages": messages,
            "stream": use_stream
        }

        try:
            if use_stream:
                # æµå¼æ¨¡å¼ï¼šä½¿ç”¨ç”Ÿæˆå™¨è¿”å›é€å—å“åº”
                yield from self._stream_response(payload)
            else:
                # éæµå¼æ¨¡å¼ï¼šç›´æ¥è¿”å›å®Œæ•´å“åº”
                yield self._full_response(payload)
        except Exception as e:
            # å¼‚å¸¸å¤„ç†
            yield f"è¯·æ±‚å¤±è´¥: {str(e)}"

    def _format_messages(self, message: str, history: List[Tuple[str, str]]) -> List[Dict[str, str]]:
        """å°†å¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºAPIæ‰€éœ€æ ¼å¼"""
        messages = []
        # éå†å†å²å¯¹è¯è®°å½•
        for human, assistant in history:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": human})
            # æ·»åŠ AIå›å¤
            messages.append({"role": "assistant", "content": assistant})
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": message})
        return messages

    def _stream_response(self, payload: Dict[str, Any]) -> Generator[str, None, None]:
        """å¤„ç†æµå¼å“åº”"""
        # å‘é€æµå¼POSTè¯·æ±‚
        with requests.post(self.chat_endpoint, json=payload, stream=True) as response:
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            partial_message = ""  # å­˜å‚¨éƒ¨åˆ†æ¶ˆæ¯
            # é€è¡Œè¯»å–æµå¼å“åº”
            for line in response.iter_lines():
                if line:
                    # è§£æJSONå“åº”å—
                    chunk = json.loads(line.decode('utf-8'))
                    if "message" in chunk and "content" in chunk["message"]:
                        content = chunk["message"]["content"]  # è·å–å†…å®¹
                        partial_message += content  # è¿½åŠ åˆ°éƒ¨åˆ†æ¶ˆæ¯
                        yield partial_message  # ç”Ÿæˆå½“å‰å®Œæ•´æ¶ˆæ¯

    def _full_response(self, payload: Dict[str, Any]) -> str:
        """å¤„ç†éæµå¼å“åº”"""
        response = requests.post(self.chat_endpoint, json=payload)  # å‘é€æ™®é€šè¯·æ±‚
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        # è¿”å›å®Œæ•´å“åº”æ¶ˆæ¯å†…å®¹
        return response.json()["message"]["content"]


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # å¯¼å…¥Gradioåº“ - ç”¨äºæ„å»ºWeb UIç•Œé¢
    import gradio as gr

    # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    client = OllamaClient()

    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="Ollama æœ¬åœ°å¯¹è¯") as app:
        # æ ‡é¢˜åŒºåŸŸ
        gr.Markdown("## ğŸ¦™ Ollama æœ¬åœ°å¤§æ¨¡å‹å¯¹è¯ç³»ç»Ÿ")

        # æ¨¡å‹é€‰æ‹©åŒºåŸŸ
        models = client.get_local_models()  # è·å–åˆå§‹æ¨¡å‹åˆ—è¡¨
        with gr.Row():  # åˆ›å»ºè¡Œå¸ƒå±€
            # æ¨¡å‹ä¸‹æ‹‰é€‰æ‹©å™¨
            model_dropdown = gr.Dropdown(
                choices=models,  # é€‰é¡¹åˆ—è¡¨
                value=models[0] if models else "",  # é»˜è®¤å€¼
                label="é€‰æ‹©æ¨¡å‹",  # æ ‡ç­¾æ–‡æœ¬
                interactive=True  # å…è®¸ç”¨æˆ·äº¤äº’
            )
            # åˆ·æ–°æŒ‰é’®
            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨")

        # èŠå¤©æ˜¾ç¤ºåŒºåŸŸ
        chatbot = gr.Chatbot(height=500)  # è®¾ç½®èŠå¤©æœºå™¨äººç»„ä»¶é«˜åº¦
        # ç”¨æˆ·è¾“å…¥æ¡†
        msg = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜...")

        # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
        with gr.Row():  # åˆ›å»ºè¡Œå¸ƒå±€
            submit_btn = gr.Button("ğŸš€ æäº¤")  # æäº¤æŒ‰é’®
            clear_btn = gr.Button("ğŸ§¹ æ¸…é™¤å¯¹è¯")  # æ¸…é™¤å¯¹è¯æŒ‰é’®
            stream_toggle = gr.Checkbox(label="æµå¼å“åº”", value=True)  # æµå¼æ¨¡å¼å¼€å…³

        # æ¨¡å‹åˆ·æ–°åŠŸèƒ½
        def refresh_models():
            """åˆ·æ–°æ¨¡å‹åˆ—è¡¨å›è°ƒå‡½æ•°"""
            new_models = client.get_local_models()
            # æ›´æ–°ä¸‹æ‹‰æ¡†çš„é€‰é¡¹å’Œå€¼
            return gr.Dropdown.update(
                choices=new_models,
                value=new_models[0] if new_models else None
            )

        # ç»‘å®šåˆ·æ–°æŒ‰é’®äº‹ä»¶
        refresh_btn.click(
            refresh_models,  # è§¦å‘å‡½æ•°
            inputs=[],  # è¾“å…¥å‚æ•°åˆ—è¡¨
            outputs=[model_dropdown]  # è¾“å‡ºç»„ä»¶
        )

        # æ¶ˆæ¯å¤„ç†å‡½æ•°
        def process_message(message, history, model_name, use_stream):
            """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”ŸæˆAIå“åº”"""
            # è°ƒç”¨ç”Ÿæˆå“åº”æ–¹æ³•ï¼ŒåŒæ—¶è·Ÿè¸ªå½“å‰æ¶ˆæ¯
            for response in client.generate_response(
                    message,
                    history,
                    model_name,
                    use_stream
            ):
                # æ„å»ºæ–°çš„èŠå¤©å†å²ï¼ˆåŒ…å«å½“å‰ç”Ÿæˆçš„å“åº”ï¼‰
                yield [(user, resp) if i != len(history) else (user, response)
                       for i, (user, resp) in enumerate(history + [(message, response)])]

        # ç»‘å®šæ¶ˆæ¯æäº¤äº‹ä»¶ï¼ˆå›è½¦æäº¤ï¼‰
        msg.submit(
            lambda: None,  # å ä½å‡½æ•°ï¼ˆç«‹å³è§¦å‘ï¼‰
            None,  # æ— è¾“å…¥
            None  # æ— è¾“å‡º
        ).then(
            process_message,  # ä¸»å¤„ç†å‡½æ•°
            [msg, chatbot, model_dropdown, stream_toggle],  # è¾“å…¥å‚æ•°
            [chatbot],  # è¾“å‡ºç»„ä»¶
        ).then(lambda: "", None, [msg])  # æ¸…ç©ºè¾“å…¥æ¡†

        # ç»‘å®šæäº¤æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        submit_btn.click(
            lambda: None,  # å ä½å‡½æ•°
            None,
            None
        ).then(
            process_message,  # ä¸»å¤„ç†å‡½æ•°
            [msg, chatbot, model_dropdown, stream_toggle],
            [chatbot],
        ).then(lambda: "", None, [msg])  # æ¸…ç©ºè¾“å…¥æ¡†

        # ç»‘å®šæ¸…é™¤å¯¹è¯æŒ‰é’®
        clear_btn.click(
            lambda: [],  # è¿”å›ç©ºåˆ—è¡¨
            None,  # æ— è¾“å…¥
            chatbot  # è¾“å‡ºåˆ°èŠå¤©ç»„ä»¶ï¼ˆé‡ç½®å†å²ï¼‰
        )
    # å¯åŠ¨åº”ç”¨
    app.launch(
        server_name="0.0.0.0", # å…è®¸æ‰€æœ‰ç½‘ç»œæ¥å£
        # server_name=server_name,  # æœ¬åœ°ä¸»æœºåœ°å€
        server_port=7960,  # æœåŠ¡ç«¯å£å·
        share=False  # ä¸ç”Ÿæˆå…¬å¼€é“¾æ¥
    )