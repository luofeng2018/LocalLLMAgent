import requests
import json
from typing import List, Tuple, Generator, Dict, Any, Optional


class OllamaClient:
    """Ollama APIå®¢æˆ·ç«¯å°è£…"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯

        Args:
            base_url: APIåŸºç¡€URL (é»˜è®¤ http://localhost:11434)
        """
        self.base_url = base_url
        self.models_endpoint = f"{base_url}/api/tags"
        self.chat_endpoint = f"{base_url}/api/chat"

    def get_local_models(self) -> List[str]:
        """è·å–æœ¬åœ°å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(self.models_endpoint)
            response.raise_for_status()
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
        except requests.exceptions.RequestException as e:
            print(f"æ¨¡å‹è·å–å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
            return ["llama3", "mistral"]
        except (json.JSONDecodeError, KeyError) as e:
            print(f"å“åº”è§£æå¤±è´¥: {str(e)}")
            return []

    def generate_response(
            self,
            message: str,
            history: List[Tuple[str, str]],
            model_name: str,
            use_stream: bool = True
    ) -> Generator[str, None, None]:
        """
        ç”Ÿæˆæ¨¡å‹å“åº” (æ”¯æŒæµå¼/éæµå¼)

        Args:
            message: ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
            history: å¯¹è¯å†å² [(ç”¨æˆ·æ¶ˆæ¯, AIå›å¤), ...]
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”

        Yields:
            AIå“åº”å†…å®¹ (åˆ†å—æˆ–å®Œæ•´)
        """
        messages = self._format_messages(message, history)

        payload = {
            "model": model_name,
            "messages": messages,
            "stream": use_stream
        }

        try:
            if use_stream:
                yield from self._stream_response(payload)
            else:
                yield self._full_response(payload)
        except Exception as e:
            yield f"è¯·æ±‚å¤±è´¥: {str(e)}"

    def _format_messages(self, message: str, history: List[Tuple[str, str]]) -> List[Dict[str, str]]:
        """å°†å¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºAPIæ‰€éœ€æ ¼å¼"""
        messages = []
        for human, assistant in history:
            messages.append({"role": "user", "content": human})
            messages.append({"role": "assistant", "content": assistant})
        messages.append({"role": "user", "content": message})
        return messages

    def _stream_response(self, payload: Dict[str, Any]) -> Generator[str, None, None]:
        """å¤„ç†æµå¼å“åº”"""
        with requests.post(self.chat_endpoint, json=payload, stream=True) as response:
            response.raise_for_status()
            partial_message = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    if "message" in chunk and "content" in chunk["message"]:
                        content = chunk["message"]["content"]
                        partial_message += content
                        yield partial_message

    def _full_response(self, payload: Dict[str, Any]) -> str:
        """å¤„ç†éæµå¼å“åº”"""
        response = requests.post(self.chat_endpoint, json=payload)
        response.raise_for_status()
        return response.json()["message"]["content"]


if __name__ == "__main__":
    # ä»¥ä¸‹ä¸ºGradioç•Œé¢ä»£ç 
    import gradio as gr

    # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    client = OllamaClient()

    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="Ollama æœ¬åœ°å¯¹è¯") as app:
        gr.Markdown("## ğŸ¦™ Ollama æœ¬åœ°å¤§æ¨¡å‹å¯¹è¯ç³»ç»Ÿ")

        # æ¨¡å‹é€‰æ‹©åŒºåŸŸ
        models = client.get_local_models()
        with gr.Row():
            model_dropdown = gr.Dropdown(
                choices=models,
                value=models[0] if models else "",
                label="é€‰æ‹©æ¨¡å‹",
                interactive=True
            )
            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹åˆ—è¡¨")

        # èŠå¤©åŒºåŸŸ
        chatbot = gr.Chatbot(height=500)
        msg = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜...")

        # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
        with gr.Row():
            submit_btn = gr.Button("ğŸš€ æäº¤")
            clear_btn = gr.Button("ğŸ§¹ æ¸…é™¤å¯¹è¯")
            stream_toggle = gr.Checkbox(label="æµå¼å“åº”", value=True)


        # æ¨¡å‹åˆ·æ–°åŠŸèƒ½
        def refresh_models():
            new_models = client.get_local_models()
            return gr.Dropdown.update(
                choices=new_models,
                value=new_models[0] if new_models else None
            )


        refresh_btn.click(
            refresh_models,
            inputs=[],
            outputs=[model_dropdown]
        )


        # æ¶ˆæ¯å¤„ç†å‡½æ•°
        def process_message(message, history, model_name, use_stream):
            for response in client.generate_response(
                    message,
                    history,
                    model_name,
                    use_stream
            ):
                yield [(user, resp) if i != len(history) else (user, response)
                       for i, (user, resp) in enumerate(history + [(message, response)])]


        # ç»‘å®šæ¶ˆæ¯æäº¤äº‹ä»¶
        msg.submit(
            lambda: None,  # ç«‹å³è§¦å‘
            None,
            None
        ).then(
            process_message,
            [msg, chatbot, model_dropdown, stream_toggle],
            [chatbot],
        ).then(lambda: "", None, [msg])

        submit_btn.click(
            lambda: None,
            None,
            None
        ).then(
            process_message,
            [msg, chatbot, model_dropdown, stream_toggle],
            [chatbot],
        ).then(lambda: "", None, [msg])

        # æ¸…é™¤å¯¹è¯
        clear_btn.click(lambda: [], None, chatbot)

    # å¯åŠ¨åº”ç”¨
    app.launch(
        server_name="127.0.0.1",
        server_port=7960,
        share=False
    )